import os
import json
import numpy as np
import tensorflow as tf
from transformers import ElectraTokenizer, TFElectraModel
from tensorflow.keras import layers, Model

# é…ç½®
MAX_LENGTH = 256
MODEL_SAVE_PATH = "electra_cnn_legal"
LOCAL_MODEL_PATH = "./hfl/chinese-legal-electra-base-disc"

# åŠ è½½ tokenizer å’Œ electra æ¨¡å‹
tokenizer = ElectraTokenizer.from_pretrained(LOCAL_MODEL_PATH)
electra_model = TFElectraModel.from_pretrained(LOCAL_MODEL_PATH)

# å®šä¹‰æ¨¡å‹ç±»
class LegalClassifier(Model):
    def __init__(self, num_classes, **kwargs):
        super().__init__(**kwargs)
        self.electra = electra_model
        self.conv1 = layers.Conv1D(128, 3, activation='gelu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))
        self.bn = layers.BatchNormalization()
        self.pool = layers.GlobalMaxPooling1D()
        self.classifier = layers.Dense(num_classes, activation='softmax')

    def call(self, inputs):
        input_ids, attention_mask = inputs
        outputs = self.electra(input_ids, attention_mask=attention_mask)
        x = outputs.last_hidden_state
        x = self.conv1(x)
        x = self.bn(x)
        x = self.pool(x)
        return self.classifier(x)

    def get_config(self):
        return {"num_classes": self.classifier.units}

    @classmethod
    def from_config(cls, config):
        return cls(**config)

# åŠ è½½ label æ˜ å°„å­—å…¸
with open("accu.txt", encoding="utf-8") as f:
    labels = [line.strip() for line in f.readlines()]
label_to_id = {label: i for i, label in enumerate(labels)}
id_to_label = {i: label for label, i in label_to_id.items()}

# åŠ è½½æ¨¡å‹
model = tf.keras.models.load_model(
    MODEL_SAVE_PATH,
    custom_objects={"LegalClassifier": LegalClassifier}
)

# é¢„æµ‹å‡½æ•°
def predict_accusation(text):
    if not text.strip():
        return "â—ï¸è¾“å…¥æ–‡æœ¬ä¸ºç©º"

    tokens = tokenizer(
        text,
        max_length=MAX_LENGTH,
        truncation=True,
        padding='max_length',
        return_tensors='tf'
    )
    input_ids = tokens["input_ids"]
    attention_mask = tokens["attention_mask"]

    preds = model.predict((input_ids, attention_mask))
    pred_id = np.argmax(preds, axis=1)[0]
    return id_to_label[pred_id]

# æ— é™å¾ªç¯é¢„æµ‹
if __name__ == "__main__":
    print("ğŸ” çŠ¯ç½ªäº‹å®ç½ªåé¢„æµ‹ç³»ç»Ÿ")
    print("è¾“å…¥çŠ¯ç½ªäº‹å®æ–‡æœ¬ï¼Œè¾“å…¥ 'Stop' å¯é€€å‡ºã€‚")
    while True:
        fact = input("\nè¯·è¾“å…¥çŠ¯ç½ªäº‹å®ï¼š\n>>> ")
        if fact.strip().lower() == "stop":
            print("ğŸ‘‹ å·²é€€å‡ºé¢„æµ‹ç³»ç»Ÿã€‚")
            break
        result = predict_accusation(fact)
        print(f"âœ… é¢„æµ‹ç½ªåï¼š{result}")
